---
layout: default
---
## <span id="about-me">About Me</span>

Hi, Iâ€™m a computer science researcher working in **computer vision** and **3D reconstruction**, with a strong interest in turning research methods into practical, easy-to-use systems. Most recently, I worked on <a href="/r3cap">R3CAP</a>, an open-source platform for real-time scene capture, editing, and collaboration across web and VR. The system combines SLAM, digital twins, and multi-user 3D editing, and was showcased at <a href="https://s2024.siggraph.org/program/labs/">SIGGRAPH 2024 Labs</a> and <a href="https://chi2025.acm.org/for-authors/interactivity">CHI 2025 Interactivity.</a>

Earlier, my PhD research focused on multi-view stereo, where I explored numerical and metaheuristic optimization techniques to tackle ill-posed problems in multi-view geometry. This work shaped my core interests in 3D vision and continues to inform how I approach newer representations and pipelines.

Over time, I have increasingly enjoyed working on systems that move beyond isolated algorithms and toward usable, end-to-end tools. My primary motivation is to develop technically sound 3D vision methods that can be deployed, evaluated, and extended in real-world settings.

I'm particularly interested in:
<ul style="margin-top:-15px; margin-left:-1em;">
  <li><b>3D reconstruction</b> (Photogrammetry, SLAM, Gaussian Splatting)</li>
  <li><b>Computer vision</b> methods for depth, geometry, and scene understanding</li>
  <li><b>Interactive spatial tools</b> for web and VR (Babylon.js, WebXR)</li>
  <li><b>Translational research</b> that moves ideas from papers into working software</li>
</ul>

---

## <span id="professional-experience">Professional Experience</span>

<h3 style="margin-bottom:0px;">Research Fellow</h3>
<p style="margin:0;"><b>Centre for Immersification, Singapore Institute of Technology</b><br>
May 2023 - July 2025 (2 years, 3 months)</p>
<ul style="margin-top:5px; margin-left:-1em;">
  <li>Co-developed an open-source platform for real-time, progressive 3D reconstruction and collaborative scene editing in web and VR.</li>
  <li>Evaluated and integrated MVS, SLAM, and Gaussian Splatting techniques to enhance system capabilities.</li>
  <li>Delivered 25+ technical presentations and live demonstrations to industry stakeholders, leading to ongoing research collaborations.</li>
  <li>Co-authored publications at peer-reviewed research venues.</li>
</ul>

---

## <span id="projects">Projects</span>

<div class="card">
  <h3>R3CAP</h3>
  <p><b>SLAM, Gaussian Splatting | Python, NeRF Studio, Babylon.js, WebXR</b></p>
  <ul style="margin-top:5px;">
    <li>Previously known as demoConstruct, R3CAP is an open-source platform that enables accessible and collaborative scene authoring, powered by progressive 3D reconstruction.</li>
    <li>Showcased at ACM SIGGRAPH 2024 and ACM CHI 2025.</li>
  </ul>
  <a href="/r3cap"><span class="card-link-spanner"></span></a>
</div>

<div class="card">
  <h3>Multi-View Stereo Reconstruction</h3>
  <p><b>3D Reconstruction, Computer Vision, Graph Cuts, Metaheuristic Optimization | Python, COLMAP, MATLAB</b></p>
  <ul style="margin-top:5px;">
    <li>A depth map-based multi-view stereo method using metaheuristics and graph cuts for depth estimation and refinement.</li>
    <li>Published in IEEE Signal Processing Letters 2022.</li>
  </ul>
  <a href="/multi-view-stereo"><span class="card-link-spanner"></span></a>
</div>

---

## <span id="publications">Publications</span>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://dl.acm.org/doi/abs/10.1145/3706599.3721190">Collaborative Scene Authoring with Near Real-Time 3D Reconstruction</a></h3>
<p style="margin:0;">Leon Foo, <b>Nirmal S. Nair</b>, Liuziyi Liu, Jeannie Su Ann Lee, Songjia Shen, Indriyati Atmosukarto, Alvin Chan, Jing Shi, Yong Joo Loh, Yih Yng Ng, Michael Chia, Chek Tien Tan<br>
<i>ACM CHI 2025 Interactivity</i> (Yokohama, Japan)</p>
<ul style="margin-left: -1.4em;">
  <!-- <li>Showcased integration of SLAM, digital twins, and collaborative 3D editing.</li> -->
</ul>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://dl.acm.org/doi/abs/10.1145/3641236.3664424">demoConstruct: Democratizing Scene Construction for Digital Twins through Progressive Reconstruction</a></h3>
<p style="margin:0;">Leon Foo, Chek Tien Tan, Liuziyi Liu, <b>Nirmal S. Nair</b>, Songjia Shen, Jeannie Lee<br>
<i>ACM SIGGRAPH 2024 Labs</i> (Denver, USA)</p>
<ul style="margin-left: -1.4em;">
  <!-- <li>Introduced demoConstruct, an open-source platform for real-time scene capture, editing, and collaboration across web and VR.</li> -->
</ul>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://doi.org/10.1109/LSP.2022.3201778">Multi-View Stereo Using Graph Cuts-Based Depth Refinement</a></h3>
<p style="margin:0;"><b>Nirmal S. Nair</b>, Madhu S. Nair<br>
<i>IEEE Signal Processing Letters</i>, 2022</p>
<ul style="margin-left: -1.4em;">
  <!-- <li>Proposed a novel graph cuts-based approach for depth map refinement in multi-view stereo reconstruction.</li> -->
</ul>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://doi.org/10.1117/12.2601119">Multi-view Stereo using Cross-view Depth Map Completion and Row-column Depth Refinement</a></h3>
<p style="margin:0;"><b>Nirmal S. Nair</b>, Madhu S. Nair<br>
<i>International Conference on Digital Image Processing, ICDIP 2021</i> (Singapore)</p>
<ul style="margin-left: -1.4em;">
</ul>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://doi.org/10.1117/12.2587241">Scalable Multi-view Stereo using CMA-ES and Distance Transform-based Depth Map Refinement</a></h3>
<p style="margin:0;"><b>Nirmal S. Nair</b>, Madhu S. Nair<br>
<i>International Conference on Machine Vision, ICMV 2020</i> (Rome, Italy)</p>
<ul style="margin-left: -1.4em;">
</ul>

<h3 style="margin-bottom:2px;font-weight:100;"><a href="https://link.springer.com/article/10.1007/s00138-020-01077-2">On Evolutionary Computation Techniques for Multi-view Triangulation</a></h3>
<p style="margin:0;"><b>Nirmal S. Nair</b>, Madhu S. Nair<br>
<i>Machine Vision and Applications</i>, Springer, 2020</p>
<ul style="margin-left: -1.4em;">
</ul>
